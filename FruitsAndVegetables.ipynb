{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitsAndVegetables.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMc/rdKv3KC9yw+Cn0qrjrn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hirenvadher954/Awesome_Machine_Learning/blob/master/FruitsAndVegetables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKOcWzS6hk7V"
      },
      "source": [
        "! Required Kaggle Api Json to Download Database"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6pFx2PKOAiU"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwWK9-cYOGph",
        "outputId": "4e746948-116b-4cf9-f08d-c2791dc0a913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "!kaggle datasets download -d moltean/fruits"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading fruits.zip to /content\n",
            " 98% 744M/760M [00:09<00:00, 53.0MB/s]\n",
            "100% 760M/760M [00:09<00:00, 80.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY-ITHnMOZU_"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/fruits.zip', 'r')\n",
        "zip_ref.extractall('files')\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXQ49m8P6ysP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo1_Kvcw6zjL"
      },
      "source": [
        "base_dir = '/content/files/fruits-360/Training'\n",
        "train_dir = '/content/files/fruits-360/Test'\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss0nVAX0AIAD",
        "outputId": "0c96ee28-5459-41d9-d4dc-b6d8decf9f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        " \n",
        "train_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    subset='training',\n",
        "    \n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    \n",
        ")\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 54190 images belonging to 131 classes.\n",
            "Found 13502 images belonging to 131 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRylxl0IV4uV"
      },
      "source": [
        "\n",
        "labels = \"\\n\".join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt','w') as f:\n",
        "  f.write(labels)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZlEfbPnWw-Y",
        "outputId": "d647875e-fe65-4fbb-b5bb-d56dbdccd440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "Img_Shape = (IMAGE_SIZE,IMAGE_SIZE,3)\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=Img_Shape,\n",
        "    include_top=False,\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "base_model.trainable = False;"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQNx59o1ad40"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.Conv2D(32,3,activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(131,activation='softmax')\n",
        "\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOFZwysUcKZS"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfArjpF5c3a2",
        "outputId": "70c9b321-7654-4fc2-a9ce-9586b085ecb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "847/847 [==============================] - 98s 116ms/step - loss: 0.6685 - accuracy: 0.8483 - val_loss: 0.1714 - val_accuracy: 0.9512\n",
            "Epoch 2/10\n",
            "847/847 [==============================] - 95s 112ms/step - loss: 0.0298 - accuracy: 0.9941 - val_loss: 0.1212 - val_accuracy: 0.9637\n",
            "Epoch 3/10\n",
            "847/847 [==============================] - 92s 108ms/step - loss: 0.0202 - accuracy: 0.9948 - val_loss: 0.1803 - val_accuracy: 0.9450\n",
            "Epoch 4/10\n",
            "847/847 [==============================] - 90s 106ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.2473 - val_accuracy: 0.9345\n",
            "Epoch 5/10\n",
            "847/847 [==============================] - 88s 104ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.2638 - val_accuracy: 0.9376\n",
            "Epoch 6/10\n",
            "847/847 [==============================] - 88s 104ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.1816 - val_accuracy: 0.9512\n",
            "Epoch 7/10\n",
            "847/847 [==============================] - 88s 104ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.1491 - val_accuracy: 0.9663\n",
            "Epoch 8/10\n",
            "847/847 [==============================] - 90s 107ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.1710 - val_accuracy: 0.9676\n",
            "Epoch 9/10\n",
            "847/847 [==============================] - 92s 108ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.2751 - val_accuracy: 0.9519\n",
            "Epoch 10/10\n",
            "847/847 [==============================] - 91s 107ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.2373 - val_accuracy: 0.9590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrb36_rdnCG",
        "outputId": "be0b45dc-5b65-47c6-ee17-a1a05d4c7d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "save_model_dir = ''\n",
        "tf.saved_model.save(model,save_model_dir)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(save_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite','wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c63OmLIYfrA7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}